{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2: Tracking User Activity\n",
    "### Maria DiMedio\n",
    "w205 Summer 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load packages for pyspark and transformation\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import udf, from_json, split, col, regexp_replace, avg, trim\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "spark1 = SparkSession.builder.appName('Ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In the following code, an unstructured data file containing information on assessments taken by our company's service will be published to Kafka, transformed and processed in Spark, and then landed in HDFS for future analysis. The steps taken to build this pipeline are annotated for future reference and replication by our Data Science team.\n",
    "\n",
    "These messages regarding assessments taken through our service are not structured in one schema, and therefore caused some limitation in the level of processing that could be done. Each of these assessment messages in Kafka record data on the type of tests taken, the scores, the user ids, timestamps, as well as nested values. In those nested values, there are additional data on the number of attemps made by a user when taking the test, the questions they got correct, incorrect, or unanswered, and some other data. The questions asked on each assessment are different from test to test, and therefore do not follow the same data schema. For this reason, these fields have been left nested, and can be extracted for further analysis on a test-by-test basis. Further explanation of this approach can be found in the report below. \n",
    "\n",
    "In the following code blocks, I will also point to 3 business questions which can be answered by this data. Those will be:\n",
    "\n",
    "1. How many people took the exam 'Introduction to Apache Spark'?\n",
    "2. What is the most common and least common exam taken through our service?\n",
    "3. How many of the assessments administered through our service allow users to have more than one attempt to take it?\n",
    "4. How many exams were administered with more than one attempt available?\n",
    "5. How many assessments were completed by users where all of the questions were answered correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the Environment to Run Kafka, Spark, and Hadoop on the VM \n",
    "\n",
    "**First, I created a docker-compose.yml file that would contain the necessary components to ingest the messages throguh Kafka, query and clean the data ingested through Spark, and land them in HDFS for further analysis and business use by potential Data Scientists. An example of that file is below:**\n",
    "\n",
    "---\n",
    "version: '3'\n",
    "\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "\n",
    "  cloudera:\n",
    "    image: midsw205/cdh-minimal:latest\n",
    "    expose:\n",
    "      - \"8020\" # nn\n",
    "      - \"50070\" # nn http\n",
    "      - \"8888\" # hue\n",
    "    #ports:\n",
    "    #- \"8888:8888\"\n",
    "\n",
    "  spark:\n",
    "    image: midsw205/spark-python:0.0.5\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    command: bash\n",
    "    depends_on:\n",
    "      - cloudera\n",
    "    environment:\n",
    "      HADOOP_NAMENODE: cloudera\n",
    "    expose:\n",
    "      - \"7000\" #jupyter notebook\n",
    "    ports:\n",
    "      - \"7000:7000\" # map instance:service\n",
    "\n",
    "  mids:\n",
    "    image: midsw205/base:latest\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then, I ran that docker image and launched spark with kafka and HDFS. The following terminal commands were used:**\n",
    "\n",
    "docker-compose up -d\n",
    "\n",
    "docker-compose logs -f kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, I created a topic 'assessments' to store the messages ingested. This allows for the messages for this data pipeline to be distinctly tied to this topic, while we have other topics also storing data not relevant to this project.**\n",
    "\n",
    "docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "    --create \\\n",
    "    --topic assessments \\\n",
    "    --partitions 1 \\\n",
    "    --replication-factor 1 \\\n",
    "    --if-not-exists \\\n",
    "    --zookeeper zookeeper:32181\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, I used kafka to read the messages in to the topic from the .json file, using kafkacat. I produced these messages in the assessments topic to ensure this pipeline was correct and could be replicated for this specific stream of user data:**\n",
    "\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-med0521/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Messages from Kafka to Spark and Clean\n",
    "The following code blocks are run to load the messages on assessments from kafka to spark to be transformed into a structured parquet file, to then land in HDFS for future use by the Data Science team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data in from kafka to spark. Use the 'assessments' topic, and make sure that you are loading all messages (from earliest to latest)\n",
    "raw_messages = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#take a look at the raw schema\n",
    "raw_messages.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the data as strings to transform further\n",
    "assessments = raw_messages.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: struct (nullable = true)\n",
      " |    |-- attempt: long (nullable = true)\n",
      " |    |-- counts: struct (nullable = true)\n",
      " |    |    |-- all_correct: boolean (nullable = true)\n",
      " |    |    |-- correct: long (nullable = true)\n",
      " |    |    |-- incomplete: long (nullable = true)\n",
      " |    |    |-- incorrect: long (nullable = true)\n",
      " |    |    |-- submitted: long (nullable = true)\n",
      " |    |    |-- total: long (nullable = true)\n",
      " |    |    |-- unanswered: long (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- questions: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- options: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- at: string (nullable = true)\n",
      " |    |    |    |    |    |-- checked: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- correct: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |    |    |-- submitted: long (nullable = true)\n",
      " |    |    |    |-- user_correct: boolean (nullable = true)\n",
      " |    |    |    |-- user_incomplete: boolean (nullable = true)\n",
      " |    |    |    |-- user_result: string (nullable = true)\n",
      " |    |    |    |-- user_submitted: boolean (nullable = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#extract messages from the string file using spark json package mapping a lambda function to pull values from the rdd\n",
    "#look at extracted schema and take note of the fields that are still nested for further investigation\n",
    "extracted_assessments1 = spark.read.json(assessments.rdd.map(lambda x: x.value))\n",
    "extracted_assessments1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#created a table in spark to be able to manipulate columns and extract those most relevant to our team.\n",
    "extracted_assessments1.registerTempTable('assessments_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#each observation in the dataset refers to an assessment taken through the service by a user. There are 3280 observations.\n",
    "extracted_assessments1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+-----------------------+\n",
      "|count(DISTINCT user_exam_id)|count(DISTINCT base_exam_id)|count(DISTINCT keen_id)|\n",
      "+----------------------------+----------------------------+-----------------------+\n",
      "|                        3242|                         107|                   3242|\n",
      "+----------------------------+----------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#EDA\n",
    "#before proceeding, take a look at the id fields and notice if they correspond to row counts.\n",
    "#from this, we can see that there are roughly 40 observations which not have a unique id based on the user_exam_id column\n",
    "df1 = extracted_assessments1.select(countDistinct(\"user_exam_id\"), countDistinct('base_exam_id'), countDistinct(\"keen_id\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|count(CASE WHEN isnan(keen_id) THEN true END)|\n",
      "+---------------------------------------------+\n",
      "|                                            0|\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This code is used to check for missing values in the unique id column. We can see that there is no missing data here, so we can \n",
    "extracted_assessments1.select([count(when(isnan('keen_id'),True))]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next, create a dataframe selecting the useful columns for our business questions from our assessments \n",
    "#pyspark dataframe using spark.sql\n",
    "\n",
    "df = spark.sql(\"select assessments_table.base_exam_id, assessments_table.certification, assessments_table.exam_name, assessments_table.max_attempts, from assessments_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|max_attempts|           sequences|\n",
      "+--------------------+-------------+--------------------+------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...|         1.0|[1,[false,2,1,1,4...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...|         1.0|[1,[false,1,2,1,4...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|         1.0|[1,[false,3,0,1,4...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|         1.0|[1,[false,2,2,0,4...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...|         1.0|[1,[false,3,0,1,4...|\n",
      "+--------------------+-------------+--------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the size of this dataframe is 5 columns x 3280 rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having extracted the columns that are useful to answer the business questions outlined above, I took a look at the size of the dataframe. There are now 5 columns, and 3280 rows. The number of observations here represents the number of unique assessments taken through our service, each with its own id. \n",
    "\n",
    "As noted above, some of the observations in this dataset have the same base_exam_id unique identifier. For that reason, when aggregating data off this spark dataframe, take note of how those repeats are handled.\n",
    "\n",
    "Next, extract the data fields from the sequences column and create new columns to include in a more detailed dataframe. First, a few exploratory sql statements are run on the nested data fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           sequences|\n",
      "+--------------------+\n",
      "|[1,[false,2,1,1,4...|\n",
      "|[1,[false,1,2,1,4...|\n",
      "|[1,[false,3,0,1,4...|\n",
      "|[1,[false,2,2,0,4...|\n",
      "|[1,[false,3,0,1,4...|\n",
      "|[1,[true,5,0,0,5,...|\n",
      "|[1,[true,1,0,0,1,...|\n",
      "|[1,[true,5,0,0,5,...|\n",
      "|[1,[true,4,0,0,4,...|\n",
      "|[1,[false,0,1,0,1...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select assessments_table.sequences from assessments_table limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           questions|\n",
      "+--------------------+\n",
      "|[[7a2ed6d3-f492-4...|\n",
      "|[[95194331-ac43-4...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select assessments_table.sequences.questions from assessments_table\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Given the nested structure of the 'counts' column, which refers to the questions answered on each exam, further unnesting can be done to extract those values into individual columns. That was done using additional spark sql querying and saved to another dataframe called 'full_df' to indicate it is has additional data for the Data Science team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = spark.sql(\"select assessments_table.base_exam_id, assessments_table.certification, assessments_table.exam_name, assessments_table.max_attempts, assessments_table.sequences.id, assessments_table.sequences.counts, assessments_table.sequences.questions, assessments_table.sequences.attempt, assessments_table.sequences.counts.all_correct, assessments_table.sequences.counts.correct, assessments_table.sequences.counts.incomplete, assessments_table.sequences.counts.incorrect, assessments_table.sequences.counts.submitted, assessments_table.sequences.counts.total, assessments_table.sequences.counts.unanswered from assessments_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- counts: struct (nullable = true)\n",
      " |    |-- all_correct: boolean (nullable = true)\n",
      " |    |-- correct: long (nullable = true)\n",
      " |    |-- incomplete: long (nullable = true)\n",
      " |    |-- incorrect: long (nullable = true)\n",
      " |    |-- submitted: long (nullable = true)\n",
      " |    |-- total: long (nullable = true)\n",
      " |    |-- unanswered: long (nullable = true)\n",
      " |-- questions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- options: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- at: string (nullable = true)\n",
      " |    |    |    |    |-- checked: boolean (nullable = true)\n",
      " |    |    |    |    |-- correct: boolean (nullable = true)\n",
      " |    |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |    |-- submitted: long (nullable = true)\n",
      " |    |    |-- user_correct: boolean (nullable = true)\n",
      " |    |    |-- user_incomplete: boolean (nullable = true)\n",
      " |    |    |-- user_result: string (nullable = true)\n",
      " |    |    |-- user_submitted: boolean (nullable = true)\n",
      " |-- attempt: long (nullable = true)\n",
      " |-- all_correct: boolean (nullable = true)\n",
      " |-- correct: long (nullable = true)\n",
      " |-- incomplete: long (nullable = true)\n",
      " |-- incorrect: long (nullable = true)\n",
      " |-- submitted: long (nullable = true)\n",
      " |-- total: long (nullable = true)\n",
      " |-- unanswered: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------+--------------------+-------------------+--------------------+-------+-----------+-------+----------+---------+---------+-----+----------+\n",
      "|        base_exam_id|certification|           exam_name|max_attempts|                  id|             counts|           questions|attempt|all_correct|correct|incomplete|incorrect|submitted|total|unanswered|\n",
      "+--------------------+-------------+--------------------+------------+--------------------+-------------------+--------------------+-------+-----------+-------+----------+---------+---------+-----+----------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...|         1.0|5b28a462-7a3b-42e...|[false,2,1,1,4,4,0]|[[7a2ed6d3-f492-4...|      1|      false|      2|         1|        1|        4|    4|         0|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...|         1.0|5b28a462-7a3b-42e...|[false,1,2,1,4,4,0]|[[95194331-ac43-4...|      1|      false|      1|         2|        1|        4|    4|         0|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|         1.0|b370a3aa-bf9e-4c1...|[false,3,0,1,4,4,0]|[[b9ff2e88-cf9d-4...|      1|      false|      3|         0|        1|        4|    4|         0|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|         1.0|b370a3aa-bf9e-4c1...|[false,2,2,0,4,4,0]|[[1f7c5def-904b-4...|      1|      false|      2|         2|        0|        4|    4|         0|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...|         1.0|04a192c1-4f5c-4ac...|[false,3,0,1,4,4,0]|[[620c924f-6bd8-1...|      1|      false|      3|         0|        1|        4|    4|         0|\n",
      "+--------------------+-------------+--------------------+------------+--------------------+-------------------+--------------------+-------+-----------+-------+----------+---------+---------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is out of the scope of this analysis, the 'questions' column now contains nested information about the users' response to questions asked in the assessments. Noteably, there are many different formats of the types of questions and response data, each with a differenct schema, so this field can be useful if the Data Science team separates this dataframe out by exam type first, and then takes a look at the question schema for each specific exam. This will be saved in this format in later steps and saved in HDFS so that the Data Science team will have access to this nested structure. \n",
    "\n",
    "For the next steps of analysis, the columns that are of interest are used and saved in both this expanded, and a focused format for the purposes of the key business questions listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "focused_df = full_df.select(\"base_exam_id\", \"certification\", \"exam_name\", \"max_attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------+\n",
      "|        base_exam_id|certification|           exam_name|max_attempts|\n",
      "+--------------------+-------------+--------------------+------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...|         1.0|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...|         1.0|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|         1.0|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|         1.0|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...|         1.0|\n",
      "+--------------------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "focused_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering Business Questions - Spark Analysis\n",
    "In the following code blocks, the three business questions posed above will be answered. This is an example of the types of analysis which the Data Science team can expect to be able to perform off of this user data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How many people took the exam 'Introduction to Apache Spark'?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from assessments_table where exam_name='Introduction to Apache Spark'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from assessments_table\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 people took the exam 'Introduction to Apache Spark' through our service, out of the 3280 total assessments administered to users in this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is the most common and least common exam taken through our service?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|           exam_name|count(exam_name)|\n",
      "+--------------------+----------------+\n",
      "|Learning to Visua...|               1|\n",
      "|Nulls, Three-valu...|               1|\n",
      "|Native Web Apps f...|               1|\n",
      "|Operating Red Hat...|               1|\n",
      "|The Closed World ...|               2|\n",
      "|Client-Side Data ...|               2|\n",
      "|Arduino Prototypi...|               2|\n",
      "|Hibernate and JPA...|               2|\n",
      "|Understanding the...|               2|\n",
      "|What's New in Jav...|               2|\n",
      "|Learning Spring P...|               2|\n",
      "|Service Based Arc...|               3|\n",
      "|Using Web Components|               3|\n",
      "|Building Web Serv...|               3|\n",
      "| Mastering Web Views|               3|\n",
      "|Getting Ready for...|               3|\n",
      "|Using Storytellin...|               4|\n",
      "|       View Updating|               4|\n",
      "|Modeling for Soft...|               5|\n",
      "|An Introduction t...|               5|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, count(exam_name) from assessments_table group by exam_name order by count(exam_name)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|           exam_name|count(exam_name)|\n",
      "+--------------------+----------------+\n",
      "|        Learning Git|             394|\n",
      "|Introduction to P...|             162|\n",
      "|Introduction to J...|             158|\n",
      "|Intermediate Pyth...|             158|\n",
      "|Learning to Progr...|             128|\n",
      "|Introduction to M...|             119|\n",
      "|Software Architec...|             109|\n",
      "|Beginning C# Prog...|              95|\n",
      "|    Learning Eclipse|              85|\n",
      "|Learning Apache M...|              80|\n",
      "|Beginning Program...|              79|\n",
      "|       Mastering Git|              77|\n",
      "|Introduction to B...|              75|\n",
      "|Advanced Machine ...|              67|\n",
      "|Learning Linux Sy...|              59|\n",
      "|JavaScript: The G...|              58|\n",
      "|        Learning SQL|              57|\n",
      "|Practical Java Pr...|              53|\n",
      "|    HTML5 The Basics|              52|\n",
      "|   Python Epiphanies|              51|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, count(exam_name) from assessments_table group by exam_name order by count(exam_name) desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these two select statements, we can see that the least common exams were tied between 'Nulls, Three-valued Logic and Missing Information', 'Learning to Visualize Data with D3.js', and 'Native Web Apps for Android'. The most popular exam is 'Learning Git', which was about twice as popular as the next most common exam taken through our service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How many of the assessments administered through our service are for certifications?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3148"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, certification from assessments_table where certification = 'false'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, certification from assessments_table where certification != false\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no assessments listed in this dataset which were administered for a certification. This would be an interesting peice of information for our potential customers, in order to target their marketing strategy and understand their customer base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. How many exams were administered with more than one attempt available?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|max_attempts|count(1)|\n",
      "+------------+--------+\n",
      "|         1.0|    3280|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select max_attempts, count(*) from assessments_table group by max_attempts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the exams administered through our service gave the user more than one attempt. This is helpful for our customers to know as they create their assessments and formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. How many assessments were completed by users where all of the questions were answered correctly?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "841"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select assessments_table.sequences.counts.all_correct from assessments_table where assessments_table.sequences.counts.all_correct = true\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all the assessments taken through our service in this data stream, 841 were completed with all questions answered correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Land Parquet Files in Hadoop for Data Scientists\n",
    "In the following code blocks, the data containing primarily unnested messages data, captured in Kafka and transformed in Spark, are saved into Parquet files and stored in HDFS. The two dataframes being saved are:\n",
    "\n",
    "1. 'full_data_frame_final1': The 'full_df'dataframe containing un-nested primary columns which can be used by the Data Science team to manipulate and extract further nested data in the 'questions' column.\n",
    "\n",
    "2. 'focused_data_frame_final': The 'df' dataframe containing focused columns that were used to answer key business questions in this report. This file omits the nested data columns for purposes when the Data Science team does not need the additional fields for focused reporting to potential clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df.write.parquet(\"/tmp/full_data_frame_final1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"/tmp/focused_data_frame_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once the files have been writen, the code below was used in the terminal to check that they had landed correctly in HDFS:**\n",
    "\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following was the result, listing the HDFS files in the terminal:**\n",
    "\n",
    "Found 4 items\n",
    "\n",
    "drwxr-xr-x   - root   supergroup          0 2021-06-28 23:01 /tmp/focused_data_frame_final\n",
    "\n",
    "drwxr-xr-x   - root   supergroup          0 2021-06-28 22:54 /tmp/full_data_frame_final1\n",
    "\n",
    "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
    "\n",
    "drwx-wx-wx   - root   supergroup          0 2021-06-23 20:46 /tmp/hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are now stored and can be used by the Data Science team!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
